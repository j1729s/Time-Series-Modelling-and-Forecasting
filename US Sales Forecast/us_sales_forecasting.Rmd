---
title: "US Sales Time Series Forecast"
output: html_document
---

```{r setup}
library('fpp2')
```

We will be using real data i.e., deflated data so that we can isolate real growth. This will prevent inflation from distorting the picture for us.

```{r data}
sales = read.csv('/home/sarfarosh/Downloads/RSXFSN.csv')
cpi = read.csv('/home/sarfarosh/Downloads/USACPIALLMINMEI.csv')
real = sales[,2]/cpi[,2]
data = ts(real,start=c(1992,1),frequency=12)
data
```

Let's start plotting the data to have look:

```{r trendplot}
autoplot(data)+ggtitle("Real US Retail Sales")+ylab("Millions in 2022 USD")+xlab('Year')
```

We have a strong positive trend in growth. We can see the dip which is probably explained by the 2008 recesion. It seems like there is some seasonality in the data. We will investigate that further.

But first let's have a look at the stationary data by taking the first difference:

```{r difference}
diff  = diff(data)
autoplot(diff)+ggtitle("Change in Real US Retail Sales")+ylab("Millions in 2022 USD")
```

We see that the data is now trend stationary.
Now we have got rid of the trend. We can see large fluctuations in the data. We need to check whether these fluctuations are regualar or irregualar. Let's create a seasonal plot for the same.

```{r seasonalplot}
ggseasonplot(diff)+ggtitle("Seasonal Plot: Change in Real US Retail Sales")+ylab("Millions in 2022 USD")
```

We can see strong seasonality in the data. There is consistent growth in November to December months succeded by a sharp fall in January and which it picks up again in February to March. This upsurge and fall can be attributed to Christman shopping. The next surge and fall can probably be explained by the upcoming summer, change in retail stock and Easter. 

Let's confirm our observationss with a subseries plot:

```{r subseriesplot}
ggsubseriesplot(diff)
```

As we can see when the data is coupled by months for each year we can confirm our observations about the surge and fall in January and other months. We can also see some rather exception growth in sales in the month of March and June in the recent times. I went back to the previuos graph to confirm what I had missed earlier. This is indicative of strong positive growth in future but also of greater volatility. We have confirmed that our data has both trend and seasonality!

This is enough for a preliminary analysis. We can now proceed with modeling and eventually making our projections based on the model of our choice. 

Let's start with the benchmark method, i.e., Naive methods. Since our data is seasonal we will go with seasonal naive, `snaive()` function which will use the corresponding season from last year as a benchmark. Mathematically, we can write this model as $y_t = y_{t-s} + \epsilon_{t}$ , where $t$ is time, $s$ is the duration going back for the benchmark, and $y_t$ is the sales in year $t$. Now because the data has a trend therefore that would  imply compounding errors for the model hence it would be better to use the first difference data i.e., stationary data for this model.

We'll fit the model, summarize it and check its residuals.

```{r naive}
fit_naive = snaive(diff)
summary(fit_naive)
checkresiduals(fit_naive)
```

We have a residual standard deviation of $125.9141$. This is our benchmark for other models that we may try. This tells us that the seasonal naive model fits the data relatively well and misses on average by about $126$ Million USD.

Cheching residuals we see that the data seems pretty random. However, looking at the ACF graph we see a lot of autocorrelation in the graph over the $95$ percent confidence intervals. This means that the model is failing to account for some information in the data. We will have to lower our auto correlation.

For our next model, we'll try out the exponential smoothing model. These are a class of time dseries forecasting model. This model with try out every single possible exponential smoothing model and it will return the best fitter. This model will test for trend and include it in the fit. Therefore, we can use the real sales data for this one. 

Let's try it out:

```{r ets}
fit_ets = ets(data)
summary(fit_ets)
checkresiduals(fit_ets)
```

The model picked by ETS is ETS(M,A,M) model i.e., the multiplicative, additive and multiplicative model. The first letter denotes the error type; the second letter denotes the trend type; and the third letter denotes the season type. So, the errors are being compounded and so is the seasonality.

This model can be described mathematically as:
$y_{t}=(l_{t-1}+b_{t-1})s_{t-m}(1+\epsilon_{t})$
$l_{t}=(l_{t-1}+b_{t-1})s_{t-m}(1+\alpha\epsilon_{t})$
$b_{t}=(l_{t-1}+b_{t-1})s_{t-m}(1+\epsilon_{t})$
$s_{t}=s_{t-m}(1+\gamma\epsilon_{t})$
where,
$\alpha, \beta, \gamma$ are the soothing parameters, and,
$l_{0},b_{0},s_{0},s_{-1},..$ are the initial states.

We can see a residual standard deviation dropped to a whopping $0.0247$, which is a massive improvement from the benchmark. It is almost too good to be true, I haven't figured out why this is so. 
Although, when it comes to auto correlation it is not an inprovement in any case if not degradation. We would like to see these within the $95$ percent confidence intervals. Although, this model performed quite well at least in sample we can try out one more model known as the ARIMA model. 

The `auto.arima()` function uses a variation of the Hyndman Khandakar algoritm which combines unit root tests, minimisation of the AICs and MLE to obtain an ARIMA model. We need stationary data for the ARIMA model therefore, we wll tell the function to take the first difference of the data we provide by supplying the constant `d=1`, we will also take the first seasonal difference by supplying `D=1`. We don't want it to approximate so we set the to `approximation=FALSE`, and setting `stepwise=FALSE` will force it to try all the models, while `trace=FALSE` implies it will not print out each model it tries but rather only the final selection.

Let's try it out:

```{r arima}
fit_arima = auto.arima(data,d=1,D=1,stepwise=FALSE,approximation=FALSE, trace=FALSE)
summary(fit_arima)
checkresiduals(fit_arima)
```

The model selected is `ARIMA(4,1,0)(0,1,1)[12]`.
This gives us a residual standard deviation of $\sqrt{8341}=91.32908$ which is certainly lower than the benchmark but much much worse than ETS model.

$ARIMA(p,d,q)(P,D,Q)_{S}$, where $(p,d,q)$ is the non seasonal part and $(P,D,Q)$ is the seasonal part of the model, and $S$ is the number of observations per year. 
Mathematically, $ARIMA(p,d,q)(P,D,Q)_{S}$ can be written as:
$$\Phi(B^{S})\phi(B)(x_{t}-\mu)=\Theta(B^{S})\theta(B)w_{t}$$
The non seasonal parts are:
AR:
$\phi(B)=1-\phi_{1}B-...-\phi_{p}B^{p}$
MA:
$\theta(B)=1+\theta_{1}B+...+\theta_{q}B^{q}$

The seasonal parts are:
Seasonal AR:
$\Phi(B)=1-\Phi_{1}B^{S}-...-\Phi_{P}B^{PS}$
Seasonal MA:
$\Theta(B)=1+\Theta_{1}B^{S}+...+\Theta_{Q}B^{QS}$

We can see quite easily here that the autocorrelation is almost within the $95$ percent confidence intervals. This is what we were looking for. Now we can proceed with forecasting the data.

We will forecast using both the ETS and ARIMA models. First, with ARIMA:

```{r forecast1}
forecast1 = forecast(fit_arima, h=24)
autoplot(forecast1)
```

And let's look at the zoomed in plot for the same.

```{r zoomedin1}
autoplot(forecast1, include = 84)
```

Now with ETS:

```{r forecast2}
forecast2 = forecast(fit_ets, h=24)
autoplot(forecast2)
```

And let's look at the zoomed in plot for the same.

```{r zoomedin2}
autoplot(forecast2, include = 84)
```

Next we can look at the summary of each forecast to conclude.

```{r summary}
summary(forecast1)
summary(forecast2)
```

I, for one, can't spot any major differences between the projections of these two models. So either of them will be useful and the choice of which one will depend on the data one is working with and the problem that one is trying to solve. ETS has a spectacularly low residual standard deviation at the expense of some auto correlation. While ARIMA minimises auto correlation for a reasonably low residual standard deviation. This concluds this project.

References:
[1] Sales data taken from: https://fred.stlouisfed.org/series/RSXFSN/
[2] CPI Data Taken from:
https://fred.stlouisfed.org/series/USACPIALLMINMEI